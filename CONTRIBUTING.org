* Working on the Scheme track

   To work the track, you should start by heading to this directory
   and firing up a Scheme REPL. Once here enter =(load "load.ss")=,
   which will orchestrate loading the appropriate files.

** Adding new problems
    
   Suppose you'd like to add =change=, a problem in the
   exercism/problem-specifications repository you think should be in
   the scheme track. The process for adding it to the track goes
   something like this:

    - =(setup-exercism 'change)= gets the specification, copying the
      default README, and creates =change.ss=, =change.scm=, and
      =example.scm= into the directory =code/exercises/change/=.
    - In =change.ss=, you need to implement two functions to turn the
      specification json into a test suite, =spec->tests= and
      =parse-test=. The job of =spec->tests= is to take the parsed
      json and find the tests. Each test is then transformed to a
      quoted, runnable scheme function by =parse-test=, which will
      end up in the generated =test.scm= file (the file that the
      student uses to check their answers). See the [[https://github.com/exercism/scheme/tree/master/code/exercises][code/exercises/*]]
      directories for examples.
    - In =example.scm=, you need to implement an example solution to
      the problem. =(verify-exercism 'change)= will check that the
      solution passes generated test suite.
    - =change.scm= will contain a stub. Ensure that the stub function
      and arguments matches the solution you provide.
    - Finally, add the problem to the configuration expression in
      [[https://github.com/exercism/scheme/blob/master/code/config.ss][code/config.ss]]. You will need to provide a uuid, which can be
      generated by the configlet binary or from the repl by calling
      =(configlet-uuid)=.

   If you're unsure about what problem to add, =(get-problem-list)=
   reads the problem-specifications repository and returns a list of
   specified problems. 

   Also important is to know that some specifications are for test
   cases which are expected to fail. To implement an instance of
   =test-success= provide the test case description, a success
   predicate, a procedure name to be tested, the input, and the
   output. For =test-error=, just a description, a procedure name,
   and the test case input suffice. Your implementation of
   =parse-test= is responsible for deciding which to use. If there's
   a problem or test cases which don't fit under this umbrella, feel
   free to raise an issue and help add the required functionality.

** Adding problems not in exercism/problem-specifications

   New problems from outside of the specifications are welcome as
   well. Develop them directly in the =exercises= directory but make
   sure to cover analogous requirements from the previous section.

